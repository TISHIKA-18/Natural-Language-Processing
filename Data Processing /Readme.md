# Data Preprocessing

This directory contains Python scripts for various data preprocessing steps required in natural language processing (NLP), specifically on Twitter datasets. The following tasks are implemented:

1. **Tokenization** - Breaking down text into individual tokens (words).
2. **Stopword Removal** - Removing commonly used words that do not carry significant meaning in sentiment analysis.
3. **Stemming/Lemmatization** - Reducing words to their root forms.

## Files:

- `tokenization.py`: Tokenizes raw text into individual words using the NLTK package.
- `stopword_removal.py`: Removes stopwords from the dataset.
- `stemming_lemmatization.py`: Applies stemming and lemmatization to normalize the text.
